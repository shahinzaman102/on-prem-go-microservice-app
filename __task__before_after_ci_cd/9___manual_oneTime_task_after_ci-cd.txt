Manual one time Task after ci/cd:
=================================

1. Get the Cluster credentials to connect and manage the GKE cluster from the Terminal
---------------------------------------------------------------------------------------

---Get the Cluster credentials to connect and manage the GKE cluster from the Terminal -->

    gcloud container clusters get-credentials go-microservice-cluster --zone us-central1


2. Checking the App with "External LoadBalancer & IP stack"
-----------------------------------------------------------

---Reserve 2 Regional External Static IP (IPv4) - named as -> microsvc, broker
    from : VPC Network -> IP addresses -> RESERVE EXTERNAL STATIC IP ADDRESS
then, put these ips into the right place inside of the files: config/__others/k8s/app-loadbalancer.yml & config/k8s/front-end.yml


Now, Deploy the [config/k8s/front-end.yml] & [config/k8s/broker.yml] files simultaneously after modifying...

- modify the file [config/k8s/front-end.yml] with - Broker url, Static image name & Service type ->  
  - Broker url : "https://broker.microsvc.net"
  - collect the static image name for the front-end service from : Artifact Registry -> Repositories --> front-end --> (collect full-name from the pull command)
    Example Static image name : us-central1-docker.pkg.dev/go-microservice-app-455110/go-microservices-repo/front-end:main-525557bb03a647e14dfe74c63148dd8236854efd
  - Service type : ClusterIP

- modify the file [config/k8s/broker.yml] with - Static image name & Service type ->  
  - collect the static image name for the front-end service from : Artifact Registry -> Repositories --> front-end --> (collect full-name from the pull command)
    Example Static image name : us-central1-docker.pkg.dev/go-microservice-app-455110/go-microservices-repo/broker-service:main-525557bb03a647e14dfe74c63148dd8236854efd
  - Service type : ClusterIP

- Deploy the modified files --> 
    cd /mnt/c/Users/shahi/Downloads/go-microservice-app/config/k8s
    kubectl apply -f front-end.yml
    kubectl apply -f broker.yml

- Now, check the status --> 
    kubectl get pods
    kubectl get svc


---Now, Deploy the "External LoadBalancer & IP stack" to access the app with ip - which can be found from "config/__others/k8s/app-loadbalancer.yml" --> 
from wsl-Terminal --> kubectl apply -f <folder_path for: app-loadbalancer.yml>
----------------
Example folder_path: C:\Users\shahi\Downloads\go-microservice-app\config\__others
                    To collect the folder_path: right click on the folder then (Copy Path)
Now, as we're running from wsl configured Terminal --> the folder_path needs to be modified like this --> 
                     /mnt/c/Users/shahi/Downloads/go-microservice-app/config/__others
  Note: The folder path needs to be modified because WSL (Windows Subsystem for Linux) uses a Linux-style file system 
      that maps Windows drives under /mnt, requiring paths to be converted accordingly.
--> So, to run any commands in the WSL-configured Terminal, we must first convert the folder path to the appropriate Linux format....
----------------
- Now, check the status --> 
    kubectl get svc
  the deployed services `microsvc-net-service` & `broker-net-service` will get assigned the Reserved IPs (microsvc & broker) as their EXTERNAL-IPs
    (wait until they are not getting assigned the Reserved IPs [microsvc & broker] - ips should be assigned within a minute)...


---Now, check the application in Browser using the IP Address of microsvc.
if Browser doesn't respond - check all the pods are running or in pending state - if in pending state - then, restart all the pods --> 
  In wsl-Terminal -->   kubectl delete pods --all
    then, check IP Address (microsvc) again...


3. Validating the resources
----------------------------

---To validate the resources from wsl-Terminal (for the created application related main resources: replicas & services after ci-cd) --> 

  kubectl get pods
  kubectl get svc
  kubectl get pvc
  kubectl get configmaps

---To validate the resources from Google Cloud Console (for all the created resources after ci-cd) --> 

  - Network Services -> Load balancing

  - Monitoring -> Alerting -> Alert policies
  - Cloud Storage -> Buckets
  - IAM & Admin -> Quotas & System Limits

  - Kubernetes Engine -> Clusters
                      -> Workloads
                      -> Secrets & ConfigMaps
                      -> Storage

  - Compute Engine -> VM instances
                  -> Disks

  - Monitoring -> Log-based metrics
               -> Logs Explorer
               -> Metrics Explorer
               -> Overview
               -> Dashboards

  Note: We can search for the desired one using the search bar. For example: "Clusters", "Recommendations" ...


4. Installing HPA
------------------

---Install HPA --> 

  Set the config-folder path like this ->   cd /mnt/c/Users/shahi/Downloads/go-microservice-app/config
  then run -->   kubectl apply -f hpa-config.yml

  Check the status -->   kubectl get hpa

---Validate the HPA --> 

  - Apply some load from the client of our app (for it - we can make some concurrent request by clicking Test Auth button...)
    (with Test Auth - it's easy to apply load on app - because it's connected with much more resources (i.e. services, configmaps, pvc) than the others...)
  - Then, monitor the hpa wsl-Terminal by --> 
      kubectl get hpa
      kubectl get pods


5. Installing the Monitoring stack with "External LoadBalancer & IP stack"
---------------------------------------------------------------------------

---Again, Reserve 2 Regional External Static IP (IPv4) - named as -> prometheus, grafana
    from : VPC Network -> IP addresses -> RESERVE EXTERNAL STATIC IP ADDRESS
then, put these ips into the right place inside of the file: config/__others/helm-monitoring-chart-loadbalancer/values.yaml

---Monitor the CPU & MEMORY requests --> kubectl top nodes

---Once CPU% & MEMORY% (cpu & memory requests) - are low or operating in normal state - then go for the below manual task --> 

- To install prometheus & grafana for the Monitoring Stack - run the below script like as we run the other scripts before --> 
   (before running the script - don't forget to modify it with necessary infos.. - like for now, set the path `helm_chart_path` - 
      for "External LoadBalancer & IP stack" - which can be found in the config/__others folder)

  10_install_prometheus_grafana.py

- Check installation --> kubectl get all -n monitoring

- also check GKE's build-in metrics server functioning or not - which collects the system & app metrics --> 
  kubectl get pods -n kube-system | grep metrics-server

---Once everything is ok - check the installation with ips of `prometheus` & `grafana` on the Browser...


uninstall: if problem arise...then to reinstall -> 
----------
helm uninstall monitoring -n monitoring
run the script again...


---Once prometheus-grafana installation is successful - deploy the service monitor configuration --> 

  Set the config-folder path like this -->   cd /mnt/c/Users/shahi/Downloads/go-microservice-app/config
  then run -->   kubectl apply -f service-monitor-config.yml
  check -->   kubectl get servicemonitors

---Now, Validate the Monitoring stack --> 
   - In Prometheus the servicemonitors should be up & running (which indicates it can scrape/collect metrics).
   - In Grafana -> 
     - First import the 3 dashboards (json files) from --> config/__others/helm-monitoring-chart-loadbalancer/files/
     - Now, propagate some metrics (i.e. input some data from Client(App) [Test functionalities], Down Third party services (i.e. Mongo, Postgres, Rabbit) -> 
        then, try to input some data from Client(App) [Test functionalities] -> so that there's error occurs,  etc.).
     - Finally, observe the Dashboards to monitor the System metrics, HPA status, Application metrics. 


6. Deleting the "External LoadBalancer & IP stack"
--------------------------------------------------

After testing the App & Monitoring with "External LoadBalancer & IP stack" - Now, we'll go for Deleting the "External LoadBalancer & IP stack"... 

---First, Uninstall the monitoring stack --> 

    helm uninstall monitoring -n monitoring

---Then, Delete the created LoadBalancers components --> 

    gcloud compute forwarding-rules list
    gcloud compute forwarding-rules delete FORWARDING_RULE_NAME --region=us-central1

    gcloud compute target-pools list
    gcloud compute target-pools delete TARGET_POOL_NAME --region=us-central1

---Finally, Delete previously Reserved all the Regional External IPs --> 

    gcloud compute addresses delete microsvc broker prometheus grafana --region=us-central1


7. Registering a domain
------------------------

---Get a domain (preferred: squarespace.com - it's by default uses the NameServers from Google) or from any Domain provider of your choice...
   - If SquareSpace then: We can get Faster & More Reliable DNS with Google's infrastructure.
   Note: We do have a Domain registered from "domains.squarespace.com" named as microsvc.net - with which we're going to work further. 
(wait for a few minutes to up to 24–48 hours for the Domain to become globally available...Domain provider should notify about it through email...
    & we can also check the status of it [Domain is Active or in Pending] from the Website of our Domain provider...)


8. Setting up DNS record for the Domains
-----------------------------------------

---Reserve 2 Global External Static IP (IPv4) - named as -> reserved-app-ip, reserved-monitoring-ip

  assuming -> reserved-app-ip:         34.120.112.87 
              reserved-monitoring-ip:  34.111.61.144

---SquareSpace -> DOMAINS -> DNS -> DNS Settings -> Custom records (we'll have 4 sub-domains `broker`, `www`, `prometheus` & `grafana`) --> 

  Host        Type    TTL        Data
  ----        -----   -----      -----
  @            A      5 mins     34.120.112.87
  broker       A      5 mins     34.120.112.87
  www          A      5 mins     34.120.112.87
  grafana      A      5 mins     34.111.61.144
  prometheus   A      5 mins 	   34.111.61.144 

Once the DNS records are correctly setup
Open git-bash or terminal to check & validate the DNS records (replace the names with the correct domain & sub-domain names) --> 

  nslookup microsvc.net
  nslookup broker.microsvc.net
  nslookup www.microsvc.net
  nslookup prometheus.microsvc.net
  nslookup grafana.microsvc.net

We can also validate the propagation of the DNS records from here -> https://dnschecker.org/all-dns-records-of-domain.php

Once the DNS records are correctly propagated (Domain pointing to the correct Reserved IP by nslookup) - move forward with point 9...


9. Getting Domain-Ingress Based HTTPS routing & External Access for our Application
------------------------------------------------------------------------------------

Apply for the GCP Managed SSL Certificate & Deploy the ingress setup for our Application simultaneously...

---Apply for GCP Managed SSL Certificate to get Secure HTTPS connection for our Application --> 

    cd /mnt/c/Users/shahi/Downloads/go-microservice-app/config
    kubectl apply -f gcp-managed-app-cert.yml

---Deploy the Ingress setup to get HTTPS routing & External Access for our Application --> 

    cd /mnt/c/Users/shahi/Downloads/go-microservice-app/config
    kubectl apply -f app-ingress.yml


Now, also quickly Deploy the [config/k8s/front-end.yml] & [config/k8s/broker.yml] files simultaneously after modifying...

- modify the file [config/k8s/front-end.yml] with - Broker url, Static image name & Service type ->  
  - Broker url : "https://broker.microsvc.net"
  - collect the static image name for the front-end service from : Artifact Registry -> Repositories --> front-end --> (collect full-name from the pull command)
    Example Static image name : us-central1-docker.pkg.dev/go-microservice-app-455110/go-microservices-repo/front-end:main-525557bb03a647e14dfe74c63148dd8236854efd
  - Service type : NodePort

- modify the file [config/k8s/broker.yml] with - Static image name & Service type ->  
  - collect the static image name for the broker service from : Artifact Registry -> Repositories --> broker-service --> (collect full-name from the pull command)
    Example Static image name : us-central1-docker.pkg.dev/go-microservice-app-455110/go-microservices-repo/broker-service:main-525557bb03a647e14dfe74c63148dd8236854efd
  - Service type : NodePort

- Deploy the modified files --> 
    cd /mnt/c/Users/shahi/Downloads/go-microservice-app/config/k8s
    kubectl apply -f front-end.yml
    kubectl apply -f broker.yml

- Now, check the status --> 
    kubectl get pods
    kubectl get svc


Now, finally check the status of the SSL Certificate & Ingress...

- Check Certificate status -->   kubectl describe managedcertificate app-cert
    - the `app-cert` can take up to 20-30 minutes to get Provisioning to Active...

- Check Ingress status -->   kubectl get ing
    - the `app-ingress` can take up to 5-10 minutes to get assigned the `reserved-app-ip` as it's EXTERNAL-IP


Once the app-ingress gets assigned the `reserved-app-ip` as it's EXTERNAL-IP - Add a Redirect Rule to the Ingress attached External LoadBalancer...

---Manually Add Redirect Rule for the Subdomain "www.microsvc.net" --> 

    Network Services -> Load balancing -> Edit (three dot icon at right most side of the Load Balancer) --> 

    Host and path rules

    Mode : select --> Advanced host and path rule (URL redirect, URL rewrite)

    ADD HOST AND PATH RULE

    Toggle "www.microsvc.net"

    Edit host and path rule

    Path (/*) --> Edit

    Action: select --> Redirect the client to different host/path

    Host redirect --> microsvc.net

    Full path redirect --> Keep this empty as it is.

    Redirect response code --> 301 - Moved Permanently (keep it as it is)

    HTTPS redirect --> Enable

    Save --> Done --> Update.


10. Testing the Application from Browser & Terminal
----------------------------------------------------

- Then, finally check the application domain securely with https --> 

  https://microsvc.net

  www.microsvc.net -> should redirect to -> https://microsvc.net


- Access the Postgres UI via port-forwarding to view and manage the database of Structured Relational important data (users) --> 

  in_terminal: kubectl port-forward service/pgadmin 8080:80
  http://localhost:8080/

  Username: admin@example.com
  Password: admin

- Access the MongoDB UI via port-forwarding to view and manage the database of Unstructured high-volume data (logs) --> 

  in_terminal: kubectl port-forward service/mongo-express 8081:8081
  http://localhost:8081

  Username: admin
  Password: password

- Access the RabbitMQ UI via port-forwarding to manage queues and messages --> 

  in_terminal: kubectl port-forward svc/rabbitmq 15672:15672
  http://localhost:15672/

  Username: guest
  Password: guest

- Access the MailHog UI via port-forwarding to capture and test emails locally --> 

  in_terminal: kubectl port-forward service/mailhog 8025:8025
  http://localhost:8025/


- Commands to test live-logs with wsl-Terminal (the pod-names need to be modified) --> 

  kubectl logs -f authentication-service-8678b78f99-66v94 | awk '{print "[auth-service] " $0}' &
  kubectl logs -f broker-service-7dcdf54d74-npmdr | awk '{print "[broker-service] " $0}' &
  kubectl logs -f front-end-6549bcc8df-gm9xj | awk '{print "[front-end] " $0}' &
  kubectl logs -f listener-service-54b4f7b4d7-7slws | awk '{print "[listener-service] " $0}' &
  kubectl logs -f logger-service-868959fc97-8mstk | awk '{print "[logger-service] " $0}' &
  kubectl logs -f mailer-service-7d44ffb74b-llrcp | awk '{print "[mailer-service] " $0}' &
  kubectl logs -f mailhog-66cbbbc4d8-xfrgt | awk '{print "[mailhog] " $0}' &
  kubectl logs -f mongo-6b68bb87c9-rmzzj | awk '{print "[mongo] " $0}' &
  kubectl logs -f postgres-bfcdb6dd5-tvkpg | awk '{print "[postgres] " $0}' &
  kubectl logs -f rabbitmq-cc9cbd446-8pqtg | awk '{print "[rabbitmq] " $0}' &

  Once the live log generation is in normal state - input data from the Client(App) & monitor the live logs in Terminal simultaneously...


11. Getting Domain-Ingress Based HTTPS routing & External Access for the Monitoring stack
------------------------------------------------------------------------------------------

Apply for the GCP Managed SSL Certificate & Deploy the ingress setup for the Monitoring stack simultaneously...

---Apply for GCP Managed SSL Certificate to get Secure HTTPS connection for the Monitoring stack --> 

    cd /mnt/c/Users/shahi/Downloads/go-microservice-app/config
    kubectl apply -f gcp-managed-monitoring-cert.yml

---Deploy the helm chart with Ingress setup to get HTTPS routing & External Access for the Monitoring stack --> 

  - Run the below script like as we run the other scripts before --> 
    (before running the script - don't forget to modify it to set the correct path `helm_chart_path` - 
      for "Domain-Ingress based setup" - which can be found in the config folder)

       10_install_prometheus_grafana.py

  - Check the installation of the Monitoring stack -->   kubectl get all -n monitoring
  
  - also validate that the previously deployed service monitors are still there or not --> kubectl get servicemonitors
    - (because service monitors are required to scrape metrics from application services)
  - also check GKE's build-in metrics server functioning or not to collect the system & app metrics --> 
       kubectl get pods -n kube-system | grep metrics-server


Now, finally check the status of the SSL Certificate & Ingress...

- Check Certificate status -->   kubectl describe managedcertificate monitoring-cert -n monitoring
    - the `monitoring-cert` can take up to 20-30 minutes to get Provisioning to Active...

- Check Ingress status -->   kubectl get ing -n monitoring
    - the `monitoring-ingress` can take up to 5-10 minutes to get assigned the `reserved-monitoring-ip` as it's EXTERNAL-IP


12. Testing the Monitoring stack from Browser
----------------------------------------------

---Check the Prometheus sub-domain securely with https --> 
    https://prometheus.microsvc.net

---Check the Grafana sub-domain securely with https --> 
    https://grafana.microsvc.net

---Now, Validate the Monitoring stack --> 
   - In Prometheus the servicemonitors should be up & running (which indicates it can scrape/collect metrics).
   - In Grafana -> 
     - First import the 3 dashboards (json files) from --> config/__others/helm-monitoring-chart-loadbalancer/files/
     - Now, propagate some metrics (i.e. input some data from Client(App) [Test functionalities], Down Third party services (i.e. Mongo, Postgres, Rabbit) -> 
        then, try to input some data from Client(App) [Test functionalities] -> so that there's error occurs,  etc.).
     - Finally, observe the Dashboards to monitor the System metrics, HPA status, Application metrics. 


13. Setting up notification channel for Grafana to receive alerts from Dashboards
----------------------------------------------------------------------------------

---Create the notification channel in Slack

  Install Slack -> SignUp -> Create Workspace -> Create a channel (ex: grafana-dashboard-alerts)
  then, on the left panel -> Add Apps -> (search) Incoming WebHooks -> Add -> it'll open the browser -> from there collect the `Webhook URL`
  it should look like --> https://hooks.slack.com/services/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

---Connect/Setup the channel in Grafana

  Grafana -> Home -> Alerting -> Contact points -> Edit

  Name: grafana-default-slack
  Integration: Slack
  Webhook URL: `your Webhook URL`

  Test -> Send -> we should receive a notification in Slack - so everything is setup correctly!

  Save contact point


14. Creating Alerting Rule in a Grafana Dashboard
--------------------------------------------------

---For crating alerting rules for any of the Dashboard panel -> From the panel menu (three dot icon) -> More... -> New alert rule -> 

   then, follow the text file called "11_alerting_rules.txt", which can be found inside the folder called : 
      __task__before_after_ci_cd from project root directory.

  - Test the Alerting --> 
    - Create an alert rule for the Panel -> Authentication Service -> Error Count, using the mentioned guide (number 2)
    - Then, Delete the MongoDB Database
    - Then, Receive an Auth Error in the Client(App) by Test Auth button
    - Then, Monitor the Alerting Rule from Grafana -> Alerting -> Alerting rules (it should turn the State: from Normal to Firing)
      And also Monitor the Error from `Authentication Service -> Error Count` Dashboard panel.
    And Meanwhile, we should have already received the Alert Notification in Slack.


15. Disable the kubelet readonly port (Optional)
-------------------------------------------------

In a private GKE cluster, the kubelet read-only port isn’t externally accessible due to private networking. 
While not strictly necessary, disabling it via gcloud enhances security best practices...

---To disable the kubelet readonly port (in Terminal) --> 

gcloud container clusters update go-microservice-cluster \
    --location=us-central1 \
    --no-enable-insecure-kubelet-readonly-port

gcloud container node-pools update custom-primary-node-pool \
    --cluster=go-microservice-cluster \
    --location=us-central1 \
    --no-enable-insecure-kubelet-readonly-port

---To check the status -->

gcloud container clusters describe go-microservice-cluster \
    --location=us-central1 \
    --flatten=nodePoolDefaults.nodeConfigDefaults \
    --format="value(nodeKubeletConfig)"

gcloud container node-pools describe custom-primary-node-pool \
    --cluster=go-microservice-cluster \
    --location=us-central1 \
    --flatten=config \
    --format="value(kubeletConfig)"


===============================================================================================================

Deleting the Resources created so far 
--------------------------------------
- Shut down the project from : IAM & Admin -> Manage Resources -> Actions menu (3 dots - at right most side of the project)
                                       -> Delete -- This will unlink the Billing account from the project and Shut it down and then the - 
                                                    project will be pending for the deletion for 30 days...
                                                    Note: Some resources (not all) can be restored along with the Project again from pending deletion...
- Or if we want to Permanently Delete all the resources created before Shuting down the project then --> 
    we can do it by manually deleting all the resources using the GCloud Console (GCP UI) & the GCloud CLI

- If we want to explore all the things this is suggested that we also be aware of how to delete the resources manually using Console & CLI...
- And also most importantly deleting the resources permanently instead of shutting down the Project for pending deletion can help to prevent -  
  --> Unexpected quota limitations in the GCP Cloud while working with multiple projects

--------------------------------  Manually delete to delete the resources permanently --------------------------------

- Delete K8s cluster --> 

    gcloud container clusters list
    gcloud container clusters delete CLUSTER_NAME --region=LOCATION

    Note: All K8s resources & the Node pools will be automatically deleted by the cluster deletion...

- Delete VM instances --> 

    gcloud compute instances list
    gcloud compute instances delete INSTANCE_NAME --zone=ZONE

- Delete Compute Disk --> 

    gcloud compute disks list
    gcloud compute disks delete DISK_NAME --zone=ZONE

- Manually delete all the "VPC firewall rules" from : Network Security --> Firewall policies

- Delete Cloud Router & Cloud NAT --> 

    gcloud compute routers nats delete nat-config --router=nat-router --region=us-central1
    gcloud compute routers delete nat-router --region=us-central1

- Delete VPC & Subnet --> 

    gcloud compute networks list
    gcloud compute networks delete NETWORK_NAME

    Note: The default subnets should be deleted by the default VPC network delete...

    gcloud compute networks subnets list
    gcloud compute networks subnets delete SUBNETWORK_NAME --region=REGION

- Delete the LoadBalancer components --> 

    gcloud compute forwarding-rules list
    gcloud compute forwarding-rules delete FORWARDING_RULE_NAME --region=REGION

    gcloud compute backend-services list
    gcloud compute backend-services delete BACKEND_SERVICE_NAME --global

    gcloud compute health-checks list
    gcloud compute health-checks delete HEALTH_CHECK_NAME

- Delete the Reserved IP addresses --> 

    gcloud compute addresses list
    gcloud compute addresses delete ADDRESS_NAME --region=REGION
    gcloud compute addresses delete ADDRESS_NAME --global

- Delete the below list Manually from Google Cloud Console --> 

    - Artifact Registry -> Repositories
    - Security -> Secret Manager -> All the created Secrets
    - Cloud Storage -> Buckets -> The Created Bucket
    - IAM & Admin -> Service Accounts -> All the Service Accounts
                  -> Workload Identity Federation -> The Created Pool & Provider
    - Security -> Certificate Manager -> SSL certificates -> The Created SSL Certificates 

- Disable All the "Enabled APIs & Services" --> 

    gcloud services list --enabled
    gcloud services list --enabled --format="value(config.name)" | xargs -I {} gcloud services disable {} --force --quiet
    gcloud services list --enabled

    gcloud services disable container.googleapis.com --force

--------------------------------------------------------------------------------------------------------------

Ensure the Deletion Manually from Google Cloud Console -->

- Kubernetes Engine -> Clusters
                    -> Workloads
                    -> Secrets & ConfigMaps
                    -> Storage

- Compute Engine -> VM instances
                 -> Disks

- Network Security -> Firewall policies

- VPC Network -> VPC networks
              -> IP addresses -> EXTERNAL IP ADDRESS

- Network Connectivity -> Cloud Router
- Network Services -> Load balancing
                   -> Cloud NAT

- Security -> Certificate Manager -> SSL certificates
- Security -> Secret Manager
- Cloud Storage -> Buckets
- Artifact Registry -> Repositories

- APIs & Services -> Enabled APIs & services

- Monitoring -> Alerting -> Edit notification channels
                         -> Alert policies

- IAM & Admin -> IAM
              -> Service Accounts
              -> Workload Identity Federation
              -> Quotas & System Limits


Unlink Billing & Delete the Project --> 

  Disable the Billing (from: Billing --> Manage billing account)
  gcloud config get project
  gcloud projects delete PROJECT_ID

--------------------------------------------------------------------------------------------------------------

But for quick test of the Project with GCP Cloud it's suggested to Shut down the project as mentioned above -> which will automatically unlink -
  the Billing account from the Project and stop incurring the cost...(the Shut down Project will be deleted after 30 days...).
